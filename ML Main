#!/usr/bin/env python
# coding: utf-8

# # <ins>Apartment Price Machine Learning</ins>
# By: Liam H, Noah W, Jay C, Sameer I

# In this project, we will be developing a pipeline to assess the value of an apartment based on a specific set of features. This dataset can be found from the kaggle website through the following link: https://www.kaggle.com/datasets/mrdaniilak/russia-real-estate-20182021
# 
# We will be using a linear regression model to assess the accuracy of our pipeline (using r-squared values)

# <b>Step 1:</b> *Import required packages*

# In[1]:


import pandas as pd
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split


# <b>Step 2:</b> *Load .csv file as a dataframe*

# In[2]:


DATA = pd.read_csv(r'C:\Users\Lhaye\OneDrive\Documents\Russian_Real_Estate info.csv')
DATA


# <u><b>Feature Definitions</b></u> - From Kaggle website
# 
# - date - date of publication of the announcement;
# - time - the time when the ad was published;
# - geo_lat - Latitude
# - geo_lon - Longitude
# - region - Region of Russia. There are 85 subjects in the country in total.
# - building_type - Facade type. 0 - Other. 1 - Panel. 2 - Monolithic. 3 - Brick. 4 - Blocky. 5 - Wooden
# - object_type - Apartment type. 1 - Secondary real estate market; 2 - New building;
# - level - Apartment floor
# - levels - Number of storeys
# - rooms - the number of living rooms. If the value is "-1", then it means "studio apartment"
# - area - the total area of ​​the apartment
# - kitchen_area - Kitchen area
# - price - Price. in rubles

# <b>Step 3:</b> *Inflation Isolation and similar asset type selection*
# 
# <u>Inflation Isolation:</u>
# 
# In order to accurately predict asset values based exclusively on their respective features, it is important for us to remove any alterations to the valuation from general appreciation in housing demand. In order to do this we will do the following steps...
# 1. Select a dataframe from only date and price columns
# 2. Group data based on month
# 3. Find average of each month grouping
# 4. Calculate percentage change from each month
# 5. Find cumulative percentage change for each month
# 6. Merge this information back with main DATA dataframe and divide listed price by respective cumulative gross adjustment
# 
# <u>Similar asset type selection:</u>
# 
# We will be using apartments from the same region (2661) and same object type (1) to help isolate for any other 'variables' not listed in the dataset.

# In[3]:


#Isolate for only object type 1 (apartment style estates) & same region (2661)
DATA = DATA.loc[DATA['object_type'] == 1]
DATA = DATA.loc[DATA['region'] == 2661]

DATASAMPLE = DATA.iloc[0:120000,:] #select a sample of the dataset (for and covid isolation) - end of dataset approximately end of 2019

#Infl. Iso FULL
InflCalc = DATA.iloc[:,0:2] # select only price and date
InflCalc['date'] = pd.to_datetime(InflCalc['date'], errors='coerce') #convert to datetime like value
InflCalc['Month'] = InflCalc['date'].dt.to_period('M') #insert new column 'Month' with only year and month (for grouping)

Series = InflCalc.groupby('Month')['price'].mean() #create series with average price per month

PCT_Change = Series.pct_change() #percentage change per month

#cumulative % to be used for comparison between base month and all other months
PCT_Change = PCT_Change.cumsum()
PCT_Change.rename("PCT_CHNG",inplace = True)

#same process as with InflCalc DF (for merging)
DATA['date'] = pd.to_datetime(DATA['date'], errors='coerce')
DATA['Month'] = DATA['date'].dt.to_period('M') 

DATA = DATA.merge(PCT_Change,how='left',left_on='Month',right_index=True) #merge

DATA['ADJ_Price'] = (DATA['price'])/((DATA['PCT_CHNG'])+1) #calculate adjusted price

#Fill in null values
DATA['ADJ_Price'].fillna(DATA['price'],inplace = True)
DATA['PCT_CHNG'].fillna(1,inplace=True)
DATA


# In[4]:


#Infl. Iso SAMPLE
InflCalcSAMPLE = DATASAMPLE.iloc[:,0:2] # select only price and date
InflCalcSAMPLE['date'] = pd.to_datetime(InflCalcSAMPLE['date'], errors='coerce') #convert to datetime like value
InflCalcSAMPLE['Month'] = InflCalcSAMPLE['date'].dt.to_period('M') #insert new column 'Month' with only year and month (for grouping)

SeriesSAMPLE = InflCalcSAMPLE.groupby('Month')['price'].mean() #create series with average price per month

PCT_ChangeSAMPLE = SeriesSAMPLE.pct_change() #percentage change per month

#cumulative % to be used for comparison between base month and all other months
PCT_ChangeSAMPLE = PCT_ChangeSAMPLE.cumsum()
PCT_ChangeSAMPLE.rename("PCT_CHNG",inplace = True)

#same process as with InflCalc DF (for merging)
DATASAMPLE['date'] = pd.to_datetime(DATASAMPLE['date'], errors='coerce')
DATASAMPLE['Month'] = DATASAMPLE['date'].dt.to_period('M') 

DATASAMPLE = DATASAMPLE.merge(PCT_ChangeSAMPLE,how='left',left_on='Month',right_index=True) #merge

DATASAMPLE['ADJ_Price'] = (DATASAMPLE['price'])/((DATASAMPLE['PCT_CHNG'])+1) #calculate adjusted price

#Fill in null values
DATASAMPLE['ADJ_Price'].fillna(DATASAMPLE['price'],inplace = True)
DATASAMPLE['PCT_CHNG'].fillna(1,inplace=True)
DATASAMPLE


# <b>Step 4:</b> *Check if any null values remain*

# In[5]:


CountFULL = DATA.isnull().sum(axis=0).sum()
CountSAMPLE = DATA.isnull().sum(axis=0).sum()
print(CountFULL)
print(CountSAMPLE)


# in our case no nulls - if there were nulls additional steps would be needed to transform null values

# <b>Step 5:</b> *Seperate values into X and Y (dependent and independents) then train test split*

# In[6]:


#Seperate values into X and Y FULL
Xf = DATA.iloc[:,3:12]
Yf = DATA.iloc[:,15]

Xf_train,Xf_test,Yf_train,Yf_test = train_test_split(Xf,Yf,test_size = 0.3,random_state = 100)

#Seperate values into X and Y SAMPLE
Xs = DATASAMPLE.iloc[:,3:12]
Ys = DATASAMPLE.iloc[:,15]

Xs_train,Xs_test,Ys_train,Ys_test = train_test_split(Xs,Ys,test_size = 0.3,random_state = 100)


# <b>Step 6:</b> *Pre-processing*
# 
# Null adjustment not needed as no null values exist in data (only standardization needed) - found above

# In[7]:


#numerical x values

#Step 1A    
from sklearn.preprocessing import StandardScaler

numX_trans = Pipeline(
    [
        ('1A',StandardScaler())
    ])


#categorical x values

#Step 1B
from sklearn.preprocessing import OneHotEncoder

catX_trans = Pipeline(
    [
        ('1B',OneHotEncoder())
    ])


# <b>Step 7:</b> *Sort between numerical and categorical X values for application of preprocessing steps*

# In[8]:


from sklearn.compose import ColumnTransformer
numX = ['geo_lat','geo_lon','level','levels','rooms','area','kitchen_area']
catX = ['building_type']

#application on numX and catX
#Step 1C
full_trans = ColumnTransformer (
    [
        ('1C_i',numX_trans,numX),
        ('1C_ii',catX_trans,catX)
    ])


# <b>Step 8:</b> *Set up training environment*

# In[9]:


from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression


# Here we will test to see which n_components value yields us the largest R Squared

# In[10]:


#full
i=2
LISTf = pd.Series([])
while i <=11:
    TrainLRf = Pipeline(
        [
            ('1Cf',full_trans),
            ('2_PCAf',PCA(n_components = i)),
            ('2_LRf',LinearRegression())
        ])

    TrainLRf.fit(Xf_train,Yf_train)
    Seriesf = pd.Series(TrainLRf.score(Xf_test,Yf_test),index=[i])
    LISTf = LISTf.append(Seriesf)
    i += 1
I_Bestf = LISTf[LISTf == LISTf.max()].index[0] # find n_componenets value that gives max score result
print('The maximum accuracy is:',LISTf[I_Bestf], ' with n_componenets =',I_Bestf)
LISTf


# In[11]:


#sample
i=2
LISTs = pd.Series([])
while i <=11:
    TrainLRs = Pipeline(
        [
            ('1Cs',full_trans),
            ('2_PCAs',PCA(n_components = i)),
            ('2_LRs',LinearRegression())
        ])

    TrainLRs.fit(Xs_train,Ys_train)
    Seriess = pd.Series(TrainLRs.score(Xs_test,Ys_test),index=[i])
    LISTs = LISTs.append(Seriess)
    i += 1
I_Bests = LISTs[LISTs == LISTs.max()].index[0] # find n_componenets value that gives max score result
print('The maximum accuracy is:',LISTs[I_Bests], ' with n_componenets =',I_Bests)
LISTs


# <b>Step 9:</b> *Use the best n_components value to run the linear regression pipeline*

# In[12]:


TrainLRFULL = Pipeline(
        [
            ('1CF',full_trans),
            ('2_PCAF',PCA(n_components = I_Bestf)),
            ('2_LRF',LinearRegression())
        ])

TrainLRFULL.fit(Xf_train,Yf_train)


# In[13]:


TrainLRSAMPLE = Pipeline(
        [
            ('1CS',full_trans),
            ('2_PCAS',PCA(n_components = I_Bests)),
            ('2_LRS',LinearRegression())
        ])

TrainLRSAMPLE.fit(Xs_train,Ys_train)


# <b>Step 10:</b> *Results*

# In[14]:


print('Linear Regression accuracy - FULL',TrainLRFULL.score(Xf_test,Yf_test))
print('Linear Regression accuracy - SAMPLE',TrainLRSAMPLE.score(Xs_test,Ys_test))


# ## Additional Analysis

# Comparative scores over n_components
# 

# In[15]:


# n_component maximization
import matplotlib.pyplot as plt
plt.plot(LISTs, label='Sample', color='green')
plt.plot(LISTf, label='Full', color='steelblue')
plt.legend(title='Dataset')
plt.ylabel('Accuracy',fontsize = 14)
plt.xlabel('n_components', fontsize = 14)
plt.title('n_component Accuracy - Sample vs. Full', fontsize = 16)
plt.show()


# Unadjusted price over time with sample data

# In[16]:


#shows average price over time SAMPLE
SeriesSAMPLE.plot(color = 'orange')
plt.ylabel('Price',fontsize = 14)
plt.xlabel('Date', fontsize = 14)
plt.title('Price over time unadjusted - SAMPLE', fontsize = 16)
plt.ylim(0, 12000000)
plt.show()


# Adjusted price over time with sample data

# In[17]:


#average price over time sample - ADJUSTED
ADJPRCSMPL = DATASAMPLE[["ADJ_Price","date"]] 
ADJPRCSMPL['Month'] = ADJPRCSMPL['date'].dt.to_period('M') 
SAMPLE_GraphADJ = ADJPRCSMPL.groupby('Month')['ADJ_Price'].mean()

SAMPLE_GraphADJ.plot(color = 'orange')
plt.ylabel('Price',fontsize = 14)
plt.xlabel('Date', fontsize = 14)
plt.title('Price over time adjusted - SAMPLE', fontsize = 16)
plt.ylim(0, 12000000)
plt.show()


# Unadjusted price over time with full data

# In[18]:


#shows average price over time FULL
Series.plot(color = 'red')
plt.ylabel('Price',fontsize = 14)
plt.xlabel('Date', fontsize = 14)
plt.title('Price over time unadjusted - FULL', fontsize = 16)
plt.ylim(0, 12000000)
plt.show()


# Adjusted price over time with full data

# In[19]:


#average price over time full - ADJUSTED
ADJPRCFL = DATA[["ADJ_Price","date"]] 
ADJPRCFL['Month'] = ADJPRCFL['date'].dt.to_period('M') 
FULL_GraphADJ = ADJPRCFL.groupby('Month')['ADJ_Price'].mean()

FULL_GraphADJ.plot(color = 'red')
plt.ylabel('Price',fontsize = 14)
plt.xlabel('Date', fontsize = 14)
plt.title('Price over time adjusted - FULL', fontsize = 16)
plt.ylim(0, 12000000)
plt.show()


# In[ ]:





# In[ ]:




